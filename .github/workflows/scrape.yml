name: Scrape the site

on:
  workflow_dispatch:
jobs:
  update:
    runs-on: ubuntu-latest
    concurrency:
      group: scrape-group
      cancel-in-progress: false
    permissions:
      contents: write
    steps:
    - uses: 'webfactory/ssh-agent@v0.9.1'
      with:
        ssh-private-key: ${{ secrets.scrape_action_private_key }}
    - uses: actions/checkout@v4
      with:
        repository: 'sib-utrecht/sib-utrecht-www-static'
        path: 'site'
        ref: 'main'
    - uses: actions/checkout@v4
      with:
        path: 'config'
    - name: Download python
      uses: actions/setup-python@v5
      with:
        python-version: '3.x'
    - name: Configure
      run: |
        python -m pip install --upgrade pip requests pytz python-dotenv
        git config --global user.email "bot@sib-utrecht.nl"
        git config --global user.name "update-bot"
    - name: Install rclone
      run: |
        sudo curl https://rclone.org/install.sh | sudo bash
        
    - name: Download pages
      run: |
        cd site
        python ../config/cache.py
      env:
        AUTH_BASIC_USER:
          ${{ vars.AUTH_BASIC_USER }}
        AUTH_BASIC_PASSWORD:
          ${{ secrets.AUTH_BASIC_PASSWORD }}
    - name: Commit and Push
      run: |
        cd site
        git add .
        git commit -m "Automatically scraped site"
        git pull
        git push

    - name: Sync static files with AWS
      run: ../config/sync_static.sh
      shell: bash
      working-directory: site
      env:
        AWS_ACCESS_KEY_ID:
          ${{ vars.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY:
          ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: eu-central-1
